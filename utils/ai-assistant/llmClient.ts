/**
 * LLM å®¢æˆ·ç«¯
 * æ”¯æŒ OpenAI å’Œ Anthropic APIï¼ŒåŒ…å«é™çº§ç­–ç•¥å’Œé”™è¯¯å¤„ç†
 */

import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import {
  LLMConfig,
  LLMMessage,
  LLMResponse,
  LLMProvider,
  LLMError,
} from '@/types/ai-assistant';
import { cacheManager } from './cacheManager';

export class LLMClient {
  private provider: LLMProvider;
  private openaiClient?: OpenAI;
  private anthropicClient?: Anthropic;
  private config: LLMConfig;
  private fallbackProvider?: LLMProvider;

  constructor(config: LLMConfig, fallbackProvider?: LLMProvider) {
    this.config = config;
    this.provider = config.provider;
    this.fallbackProvider = fallbackProvider;

    // åˆå§‹åŒ–å®¢æˆ·ç«¯
    this.initializeClients();
  }

  /**
   * åˆå§‹åŒ– API å®¢æˆ·ç«¯
   */
  private initializeClients(): void {
    try {
      if (this.provider === 'openai') {
        this.openaiClient = new OpenAI({
          apiKey: this.config.apiKey,
          dangerouslyAllowBrowser: true, // ä»…ç”¨äºå¼€å‘ï¼Œç”Ÿäº§ç¯å¢ƒåº”è¯¥é€šè¿‡åç«¯è°ƒç”¨
        });
      } else if (this.provider === 'qwen') {
        // é€šä¹‰åƒé—®ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
        this.openaiClient = new OpenAI({
          apiKey: this.config.apiKey,
          baseURL: process.env.NEXT_PUBLIC_QWEN_BASE_URL || 'https://dashscope.aliyuncs.com/compatible-mode/v1',
          dangerouslyAllowBrowser: true,
        });
      } else if (this.provider === 'anthropic') {
        this.anthropicClient = new Anthropic({
          apiKey: this.config.apiKey,
        });
      }
    } catch (error) {
      console.error('Failed to initialize LLM client:', error);
      throw new LLMError('Failed to initialize LLM client', { error });
    }
  }

  /**
   * å‘é€èŠå¤©è¯·æ±‚
   */
  async chat(
    messages: LLMMessage[],
    options?: {
      useCache?: boolean;
      stream?: boolean;
      onChunk?: (chunk: string) => void;
    }
  ): Promise<LLMResponse> {
    const { useCache = true, stream = false, onChunk } = options || {};

    // æ£€æŸ¥ç¼“å­˜
    if (useCache && !stream) {
      const cacheKey = this.generateCacheKey(messages);
      const cached = cacheManager.get<LLMResponse>(cacheKey);
      if (cached) {
        console.log('[LLMClient] Cache hit:', cacheKey);
        return cached;
      }
    }

    try {
      let response: LLMResponse;

      if (stream && onChunk) {
        response = await this.streamChat(messages, onChunk);
      } else {
        response = await this.executeChat(messages);
      }

      // ç¼“å­˜ç»“æœ
      if (useCache && !stream) {
        const cacheKey = this.generateCacheKey(messages);
        cacheManager.set(cacheKey, response, 30 * 60 * 1000); // 30åˆ†é’Ÿ
      }

      return response;
    } catch (error) {
      console.error('[LLMClient] Chat failed:', error);
      return await this.handleError(error, messages);
    }
  }

  /**
   * æ‰§è¡ŒèŠå¤©è¯·æ±‚ï¼ˆéæµå¼ï¼‰
   */
  private async executeChat(messages: LLMMessage[]): Promise<LLMResponse> {
    if (this.provider === 'openai' || this.provider === 'qwen') {
      // é€šä¹‰åƒé—®å’ŒOpenAIä½¿ç”¨ç›¸åŒçš„è°ƒç”¨é€»è¾‘
      return await this.chatWithOpenAI(messages);
    } else if (this.provider === 'anthropic') {
      return await this.chatWithAnthropic(messages);
    } else {
      throw new LLMError(`Unsupported provider: ${this.provider}`);
    }
  }

  /**
   * OpenAI èŠå¤©
   */
  private async chatWithOpenAI(messages: LLMMessage[]): Promise<LLMResponse> {
    if (!this.openaiClient) {
      throw new LLMError('OpenAI client not initialized');
    }

    const response = await this.openaiClient.chat.completions.create({
      model: this.config.model,
      messages: messages as any,
      max_tokens: this.config.maxTokens || 2000,
      temperature: this.config.temperature || 0.7,
    });

    const choice = response.choices[0];
    if (!choice || !choice.message) {
      throw new LLMError('Invalid OpenAI response');
    }

    return {
      content: choice.message.content || '',
      model: response.model,
      tokens: {
        prompt: response.usage?.prompt_tokens || 0,
        completion: response.usage?.completion_tokens || 0,
        total: response.usage?.total_tokens || 0,
      },
      finishReason: choice.finish_reason,
    };
  }

  /**
   * Anthropic èŠå¤©
   */
  private async chatWithAnthropic(messages: LLMMessage[]): Promise<LLMResponse> {
    if (!this.anthropicClient) {
      throw new LLMError('Anthropic client not initialized');
    }

    // åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯
    const systemMessage = messages.find((m) => m.role === 'system');
    const userMessages = messages.filter((m) => m.role !== 'system');

    const response = await this.anthropicClient.messages.create({
      model: this.config.model,
      max_tokens: this.config.maxTokens || 2000,
      temperature: this.config.temperature || 0.7,
      system: systemMessage?.content,
      messages: userMessages.map((m) => ({
        role: m.role === 'user' ? 'user' : 'assistant',
        content: m.content,
      })),
    });

    const content = response.content[0];
    if (content.type !== 'text') {
      throw new LLMError('Invalid Anthropic response type');
    }

    return {
      content: content.text,
      model: response.model,
      tokens: {
        prompt: response.usage.input_tokens,
        completion: response.usage.output_tokens,
        total: response.usage.input_tokens + response.usage.output_tokens,
      },
      finishReason: response.stop_reason || undefined,
    };
  }

  /**
   * æµå¼èŠå¤©
   */
  private async streamChat(
    messages: LLMMessage[],
    onChunk: (chunk: string) => void
  ): Promise<LLMResponse> {
    let fullContent = '';
    let totalTokens = 0;

    if (this.provider === 'openai' && this.openaiClient) {
      const stream = await this.openaiClient.chat.completions.create({
        model: this.config.model,
        messages: messages as any,
        max_tokens: this.config.maxTokens || 2000,
        temperature: this.config.temperature || 0.7,
        stream: true,
      });

      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || '';
        if (content) {
          fullContent += content;
          onChunk(content);
        }
      }
    } else if (this.provider === 'anthropic' && this.anthropicClient) {
      const systemMessage = messages.find((m) => m.role === 'system');
      const userMessages = messages.filter((m) => m.role !== 'system');

      const stream = await this.anthropicClient.messages.create({
        model: this.config.model,
        max_tokens: this.config.maxTokens || 2000,
        temperature: this.config.temperature || 0.7,
        system: systemMessage?.content,
        messages: userMessages.map((m) => ({
          role: m.role === 'user' ? 'user' : 'assistant',
          content: m.content,
        })),
        stream: true,
      });

      for await (const event of stream) {
        if (event.type === 'content_block_delta' && event.delta.type === 'text_delta') {
          const content = event.delta.text;
          fullContent += content;
          onChunk(content);
        }
      }
    }

    return {
      content: fullContent,
      model: this.config.model,
      tokens: {
        prompt: 0,
        completion: 0,
        total: totalTokens,
      },
    };
  }

  /**
   * é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥
   */
  private async handleError(
    error: any,
    messages: LLMMessage[]
  ): Promise<LLMResponse> {
    console.error('[LLMClient] Error details:', error);

    // ç­–ç•¥1: å°è¯•å¤‡ç”¨ LLM
    if (this.fallbackProvider && this.fallbackProvider !== this.provider) {
      console.log(`[LLMClient] Trying fallback provider: ${this.fallbackProvider}`);
      try {
        const originalProvider = this.provider;
        this.provider = this.fallbackProvider;
        this.initializeClients();
        const response = await this.executeChat(messages);
        this.provider = originalProvider;
        this.initializeClients();
        return response;
      } catch (fallbackError) {
        console.error('[LLMClient] Fallback provider also failed:', fallbackError);
      }
    }

    // ç­–ç•¥2: æ£€æŸ¥ç¼“å­˜ï¼ˆå¿½ç•¥æ—¶æ•ˆï¼‰
    const cacheKey = this.generateCacheKey(messages);
    const cachedResponse = cacheManager.get<LLMResponse>(cacheKey);
    if (cachedResponse) {
      console.log('[LLMClient] Using stale cache due to error');
      return cachedResponse;
    }

    // ç­–ç•¥3: è¿”å›é™çº§å“åº”
    throw new LLMError('LLM request failed and no fallback available', {
      originalError: error,
      provider: this.provider,
    });
  }

  /**
   * ç”Ÿæˆç¼“å­˜é”®
   */
  private generateCacheKey(messages: LLMMessage[]): string {
    const messageHash = messages
      .map((m) => `${m.role}:${m.content}`)
      .join('|');
    return `llm:${this.provider}:${this.config.model}:${this.hashString(messageHash)}`;
  }

  /**
   * ç®€å•çš„å­—ç¬¦ä¸²å“ˆå¸Œå‡½æ•°
   */
  private hashString(str: string): string {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = (hash << 5) - hash + char;
      hash = hash & hash; // Convert to 32bit integer
    }
    return Math.abs(hash).toString(36);
  }

  /**
   * è·å–ç»Ÿè®¡ä¿¡æ¯
   */
  getStats() {
    return {
      provider: this.provider,
      model: this.config.model,
      cacheStats: cacheManager.getStats(),
    };
  }
}

/**
 * åˆ›å»º LLM å®¢æˆ·ç«¯å®ä¾‹
 * ğŸ”¥ å®‰å…¨æ”¹è¿›ï¼šç”Ÿäº§ç¯å¢ƒä½¿ç”¨æœåŠ¡ç«¯ APIï¼Œå¼€å‘ç¯å¢ƒä½¿ç”¨å®¢æˆ·ç«¯ç›´è¿
 */
export function createLLMClient(): LLMClient | null {
  try {
    // ğŸ”¥ ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨å®‰å…¨çš„æœåŠ¡ç«¯ API
    if (process.env.NODE_ENV === 'production' || process.env.NEXT_PUBLIC_ENV === 'production') {
      console.log('[LLMClient] Using secure server-side API in production');
      // è¿”å› nullï¼Œè®©è°ƒç”¨æ–¹ä½¿ç”¨ secureLLMClient
      return null;
    }

    // å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨å®¢æˆ·ç«¯ç›´è¿ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰
    const provider = (process.env.NEXT_PUBLIC_LLM_PROVIDER || 'openai') as LLMProvider;

    let apiKey: string | undefined;
    let model: string;

    if (provider === 'qwen') {
      apiKey = process.env.NEXT_PUBLIC_QWEN_API_KEY;
      model = process.env.NEXT_PUBLIC_LLM_MODEL || 'qwen-plus';
    } else if (provider === 'openai') {
      apiKey = process.env.NEXT_PUBLIC_OPENAI_API_KEY;
      model = process.env.NEXT_PUBLIC_LLM_MODEL || 'gpt-4-turbo';
    } else if (provider === 'anthropic') {
      apiKey = process.env.NEXT_PUBLIC_ANTHROPIC_API_KEY;
      model = process.env.NEXT_PUBLIC_LLM_MODEL || 'claude-3-5-sonnet-20241022';
    } else {
      apiKey = undefined;
      model = 'unknown';
    }

    if (!apiKey) {
      console.warn('[LLMClient] No API key configured for development, will use server-side API');
      return null;
    }

    const config: LLMConfig = {
      provider,
      apiKey,
      model,
      maxTokens: parseInt(process.env.NEXT_PUBLIC_LLM_MAX_TOKENS || '2000'),
      temperature: parseFloat(process.env.NEXT_PUBLIC_LLM_TEMPERATURE || '0.7'),
    };

    // é™çº§ç­–ç•¥ï¼šqwen â†’ openai â†’ anthropic
    const fallbackProvider = provider === 'qwen' ? 'openai' :
                            provider === 'openai' ? 'anthropic' : 'openai';

    return new LLMClient(config, fallbackProvider);
  } catch (error) {
    console.error('[LLMClient] Failed to create client:', error);
    return null;
  }
}

// å¯¼å‡ºå•ä¾‹å®ä¾‹ï¼ˆå¼€å‘ç¯å¢ƒå¯èƒ½æœ‰å€¼ï¼Œç”Ÿäº§ç¯å¢ƒä¸º nullï¼‰
export const llmClient = createLLMClient();

// ğŸ”¥ å¯¼å‡ºå®‰å…¨çš„å®¢æˆ·ç«¯ï¼ˆç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼‰
export { secureLLMClient } from './secureLLMClient';
